{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lucid: Workload Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicate key in file PosixPath('/home/lihe/Software/Anaconda3/envs/lucid/lib/python3.9/site-packages/matplotlib/mpl-data/matplotlibrc'), line 271 ('font.sans-serif: DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing, metrics\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "font = {\n",
    "#     \"font.family\": \"Roboto\",\n",
    "    \"font.size\": 12,\n",
    "}\n",
    "sns.set_style(font)\n",
    "paper_rc = {\n",
    "    \"lines.linewidth\": 3,\n",
    "    \"lines.markersize\": 10,\n",
    "}\n",
    "sns.set_context(\"paper\", font_scale=1.8, rc=paper_rc)\n",
    "current_palette = sns.color_palette()\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "idx = 0\n",
    "save = False\n",
    "experiment_list = [\"Venus_Sept\", \"Saturn_Sept\", \"Philly\"]\n",
    "cluster_list = [\"Venus\", \"Saturn\", \"Philly\"]\n",
    "cluster = cluster_list[idx]\n",
    "experiment = experiment_list[idx]\n",
    "\n",
    "datapath = f\"../data/{cluster}\"\n",
    "\n",
    "\n",
    "result = pd.DataFrame()\n",
    "if cluster == \"Philly\":\n",
    "    df = pd.read_csv(\n",
    "        f\"{datapath}/cluster_full_log.csv\",\n",
    "        parse_dates=[\"submit_time\"],\n",
    "        usecols=[\n",
    "            \"job_id\",\n",
    "            \"user\",\n",
    "            \"vc\",\n",
    "            \"gpu_num\",\n",
    "            \"submit_time\",\n",
    "            \"amp\",\n",
    "            \"gpu_util\",\n",
    "            \"gmem_util\",\n",
    "            \"gmem\",\n",
    "            \"duration\",\n",
    "        ],\n",
    "    )\n",
    "else:\n",
    "    df = pd.read_csv(\n",
    "        f\"{datapath}/cluster_full_log.csv\",\n",
    "        parse_dates=[\"submit_time\"],\n",
    "        usecols=[\n",
    "            \"job_id\",\n",
    "            \"user\",\n",
    "            \"vc\",\n",
    "            # \"jobname\",\n",
    "            \"gpu_num\",\n",
    "            \"cpu_num\",\n",
    "            \"submit_time\",\n",
    "            \"month\",\n",
    "            \"day\",\n",
    "            \"hour\",\n",
    "            \"dayofweek\",\n",
    "            \"amp\",\n",
    "            \"gpu_util\",\n",
    "            \"gmem_util\",\n",
    "            \"gmem\",\n",
    "            \"duration\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "if cluster == \"Philly\":\n",
    "    trace_range = (\"2017-10-01 00:00:00\", \"2017-10-07 23:59:00\")\n",
    "    train_df = df[(df[\"submit_time\"] > trace_range[1])]\n",
    "    val_df = df[(df[\"submit_time\"] >= trace_range[0]) & (df[\"submit_time\"] <= trace_range[1])]\n",
    "else:\n",
    "    # trace_range = (\"2020-09-01 00:00:00\", \"2020-09-26 23:59:59\")\n",
    "    trace_range = (\"2020-09-01 00:00:00\", \"2020-09-27 00:10:00\")  # Add a bit more job for prediction\n",
    "    train_df = df[(df[\"submit_time\"] < trace_range[0])]\n",
    "    val_df = df[(df[\"submit_time\"] >= trace_range[0]) & (df[\"submit_time\"] <= trace_range[1])]\n",
    "\n",
    "\n",
    "train_df = train_df.sort_values(by=\"submit_time\")\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "val_df = val_df.sort_values(by=\"submit_time\")\n",
    "val_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "train_data = train_df.drop(columns=[\"duration\", \"submit_time\"])\n",
    "test_data = val_df.drop(columns=[\"duration\", \"submit_time\"])\n",
    "train_label = train_df[[\"duration\"]]\n",
    "test_label = val_df[[\"duration\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weekly Update Lucid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据长度: 89556\n",
      "训练结束\n",
      "平均绝对误差分数: 11573.96, 平均绝对误差百分比分数: 121.86, R2分数: 0.4459\n",
      "训练数据长度: 94471\n",
      "训练结束\n",
      "平均绝对误差分数: 13560.14, 平均绝对误差百分比分数: 345.90, R2分数: 0.4224\n",
      "训练数据长度: 101093\n",
      "训练结束\n",
      "平均绝对误差分数: 13327.78, 平均绝对误差百分比分数: 102.34, R2分数: 0.2240\n",
      "训练数据长度: 107707\n",
      "训练结束\n",
      "平均绝对误差分数: 11814.87, 平均绝对误差百分比分数: 222.77, R2分数: 0.5189\n"
     ]
    }
   ],
   "source": [
    "trace_range_list = [\n",
    "    (\"2020-09-01 00:00:00\", \"2020-09-07 00:00:00\"), # Week 1\n",
    "    (\"2020-09-07 00:00:00\", \"2020-09-14 00:00:00\"), # Week 2\n",
    "    (\"2020-09-14 00:00:00\", \"2020-09-21 00:00:00\"), # Week 3\n",
    "    (\"2020-09-21 00:00:00\", \"2020-09-27 00:10:00\"), # Week 4\n",
    "]\n",
    "week_df = pd.DataFrame()\n",
    "for trace_range in trace_range_list:\n",
    "\n",
    "    train_df = df[(df[\"submit_time\"] < trace_range[0])]\n",
    "    val_df = df[(df[\"submit_time\"] >= trace_range[0]) & (df[\"submit_time\"] <= trace_range[1])]\n",
    "\n",
    "\n",
    "    train_df = train_df.sort_values(by=\"submit_time\")\n",
    "    train_df.reset_index(inplace=True, drop=True)\n",
    "    val_df = val_df.sort_values(by=\"submit_time\")\n",
    "    val_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    train_data = train_df.drop(columns=[\"duration\", \"submit_time\"])\n",
    "    test_data = val_df.drop(columns=[\"duration\", \"submit_time\"])\n",
    "    train_label = train_df[[\"duration\"]]\n",
    "    test_label = val_df[[\"duration\"]]\n",
    "\n",
    "    print(f\"训练数据长度: {len(train_data)}\")\n",
    "# , binning=\"uniform\"\n",
    "    ebm = ExplainableBoostingRegressor(learning_rate=0.01, interactions=20)\n",
    "    ebm.fit(train_data, train_label)\n",
    "    print('训练结束')\n",
    "    pred = ebm.predict(test_data)\n",
    "\n",
    "    mae_score = metrics.mean_absolute_error(test_label, pred)\n",
    "    mape_score = metrics.mean_absolute_percentage_error(test_label, pred)\n",
    "    r2_score = metrics.r2_score(test_label, pred)\n",
    "    result.at[\"ebm_r2\", cluster] = r2_score\n",
    "    print(f\"平均绝对误差分数: {mae_score:.2f}, 平均绝对误差百分比分数: {mape_score:.2f}, R2分数: {r2_score:.4f}\")\n",
    "\n",
    "    pred = pred.astype(int)\n",
    "    val_df.loc[:,'priority'] = pred\n",
    "    week_df = pd.concat([week_df, val_df])\n",
    "# week_df.to_csv(f\"ebm/{experiment}_Sept_ebm_weekly_updated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Name Affinity Propagation\n",
    "\n",
    "Scripts below need original jobname information, which cannot release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        NaN\n",
      "1        NaN\n",
      "2        NaN\n",
      "3        NaN\n",
      "4        NaN\n",
      "          ..\n",
      "114816   NaN\n",
      "114817   NaN\n",
      "114818   NaN\n",
      "114819   NaN\n",
      "114820   NaN\n",
      "Name: jobname, Length: 114821, dtype: float64\n",
      "Series([], Name: count, dtype: int64)\n",
      "[]\n",
      "finished\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import distance\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "top_high_freq_num = 100\n",
    "\n",
    "idx = 0\n",
    "experiment_list = [\"Venus_Sept\", \"Saturn_Sept\", \"Philly\"]\n",
    "cluster_list = [\"Venus\", \"Saturn\", \"Philly\"]\n",
    "cluster = cluster_list[idx]\n",
    "experiment = experiment_list[idx]\n",
    "\n",
    "df = pd.read_csv(f'../data/{cluster}/cluster_full_log.csv',\n",
    "                 parse_dates=['submit_time', 'start_time', 'end_time'])\n",
    "\n",
    "df = df[df['gpu_num']>0]\n",
    "df.drop(columns=['year', 'nodelist', 'priority', 'minute'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "count = df['jobname'].value_counts()\n",
    "print(count)\n",
    "name_list = list(count.index)\n",
    "print(name_list)\n",
    "\n",
    "high_freq = name_list[:top_high_freq_num]\n",
    "to_cluster = name_list[top_high_freq_num:]\n",
    "to_cluster.sort()\n",
    "\n",
    "groups = [list(g) for k, g in groupby(to_cluster, key=lambda x: x[0])]\n",
    "print('finished')\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "\n",
    "for group in groups:\n",
    "    if len(group) == 1:\n",
    "        label_dict.update({group[0]:group[0]})\n",
    "    else:\n",
    "        print(f\"Processing First Character: {group[0][0]}, Lenth: {len(group)}\")\n",
    "        ts = time.time()\n",
    "\n",
    "        names = np.asarray(group)\n",
    "        lev_similarity = -1 * np.array([[distance.levenshtein(w1, w2) for w1 in names] for w2 in names])\n",
    "\n",
    "        affprop = AffinityPropagation(affinity=\"precomputed\", damping=0.9, random_state=6)\n",
    "        affprop.fit(lev_similarity)\n",
    "\n",
    "        for cluster_id in np.unique(affprop.labels_):\n",
    "            exemplar = names[affprop.cluster_centers_indices_[cluster_id]]\n",
    "            cluster = np.unique(names[np.nonzero(affprop.labels_==cluster_id)])\n",
    "\n",
    "            for ori in cluster:\n",
    "                label_dict.update({ori:exemplar})\n",
    "\n",
    "        print(f\"Time Cost: {time.time()-ts} s\")\n",
    "\n",
    "for i in high_freq:\n",
    "    label_dict.update({i:i})\n",
    "assert len(label_dict) == len(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "np.float64(nan)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Replace Name\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[0;32m----> 3\u001b[0m     df\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjobname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/cluster_full_log.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyError\u001b[0m: np.float64(nan)"
     ]
    }
   ],
   "source": [
    "\"\"\"Replace Name\"\"\"\n",
    "for i in range(len(df)):\n",
    "    df.at[i, 'jobname'] = label_dict[df.at[i, 'jobname']]\n",
    "\n",
    "df.to_csv(f\"./{cluster}/cluster_full_log.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lucid-kernel",
   "language": "python",
   "name": "lucid-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "784914556ea7aafefbdcb0c4fefea2600a01efb6a6b7916d4154dc17a3e6434f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
